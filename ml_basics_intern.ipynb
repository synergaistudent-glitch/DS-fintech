{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (uncomment for local runs)\n",
        "# !pip install numpy pandas scikit-learn matplotlib seaborn\n",
        "\n",
        "# Import libraries and print versions for reproducibility\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import matplotlib\n",
        "import seaborn\n",
        "\n",
        "print(f'Python version: {sys.version}')\n",
        "print(f'numpy version: {np.__version__}')\n",
        "print(f'pandas version: {pd.__version__}')\n",
        "print(f'scikit-learn version: {sklearn.__version__}')\n",
        "print(f'matplotlib version: {matplotlib.__version__}')\n",
        "print(f'seaborn version: {seaborn.__version__}')\n",
        "\n",
        "# Set random state for reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "\n",
        "**Machine Learning (ML)** is a field of artificial intelligence where models learn patterns from data to make predictions or decisions. In *supervised learning*, models are trained on labeled data, where inputs (features) are paired with known outputs (targets). This contrasts with *unsupervised learning*, where the data has no labels, and the model identifies patterns or structures.\n",
        "\n",
        "### Goals\n",
        "- Demonstrate supervised learning workflows for regression and classification.\n",
        "- Cover data preprocessing, model training, evaluation, and visualization.\n",
        "- Provide practical experience with scikit-learn Pipelines and common ML practices.\n",
        "\n",
        "### Datasets\n",
        "- **Regression**: California Housing dataset (from scikit-learn). Predicts median house values based on features like median income and house age. Chosen for its real-world relevance and continuous target.\n",
        "- **Classification**: Breast Cancer Wisconsin dataset (from scikit-learn). Predicts whether a tumor is malignant or benign based on cell measurements. Chosen for its binary classification task and medical context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data\n",
        "\n",
        "We load and explore two datasets: California Housing (regression) and Breast Cancer Wisconsin (classification)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing, load_breast_cancer\n",
        "\n",
        "# Load regression dataset\n",
        "california = fetch_california_housing()\n",
        "X_reg = pd.DataFrame(california.data, columns=california.feature_names)\n",
        "y_reg = california.target\n",
        "\n",
        "print('California Housing Dataset:')\n",
        "print(f'Shape: {X_reg.shape}')\n",
        "print(f'Feature Names: {california.feature_names}')\n",
        "print(f'Target: Median house value (in $100,000s)')\n",
        "print('\\nFirst 5 rows:')\n",
        "print(X_reg.head())\n",
        "print('\\nSummary statistics:')\n",
        "print(X_reg.describe())\n",
        "print('\\nMissing values:')\n",
        "print(X_reg.isna().sum())\n",
        "\n",
        "# Load classification dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X_clf = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "y_clf = breast_cancer.target\n",
        "\n",
        "print('\\nBreast Cancer Wisconsin Dataset:')\n",
        "print(f'Shape: {X_clf.shape}')\n",
        "print(f'Feature Names: {breast_cancer.feature_names}')\n",
        "print(f'Target: {breast_cancer.target_names} (0=malignant, 1=benign)')\n",
        "print('\\nFirst 5 rows:')\n",
        "print(X_clf.head())\n",
        "print('\\nSummary statistics:')\n",
        "print(X_clf.describe())\n",
        "print('\\nMissing values:')\n",
        "print(X_clf.isna().sum())\n",
        "\n",
        "# Check class distribution for classification\n",
        "print('\\nClass distribution (Breast Cancer):')\n",
        "print(pd.Series(y_clf).value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Methods\n",
        "\n",
        "### Overview\n",
        "- **Train/Validation/Test Splits**: We split data into training (70%), validation (15%), and test (15%) sets. The training set is used to fit the model, the validation set to tune hyperparameters, and the test set for final evaluation. This prevents overfitting (model memorizing training data) and underfitting (model too simple to capture patterns).\n",
        "- **Preprocessing**: We use scikit-learn Pipelines to handle missing values, encode categorical features (if any), and scale numerical features. Pipelines ensure no data leakage (e.g., scaling using test data statistics).\n",
        "- **Models**: LinearRegression for regression, LogisticRegression for classification (chosen for simplicity and interpretability).\n",
        "\n",
        "### Preprocessing Pipeline\n",
        "- Handle missing values using SimpleImputer (though datasets have no missing values, we include for demonstration).\n",
        "- No categorical features in these datasets, so we skip encoding but include a placeholder example.\n",
        "- Apply StandardScaler to normalize numerical features (essential for LogisticRegression).\n",
        "\n",
        "### Data Splitting\n",
        "- Split data into train/validation/test sets.\n",
        "- Stratify classification split to maintain class proportions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Placeholder: Synthetic categorical column for demonstration\n",
        "X_reg['synthetic_cat'] = pd.qcut(X_reg['MedInc'], q=3, labels=['low', 'medium', 'high'])\n",
        "X_clf['synthetic_cat'] = pd.qcut(X_clf['mean radius'], q=3, labels=['small', 'medium', 'large'])\n",
        "\n",
        "# Define numerical and categorical columns\n",
        "num_features_reg = [col for col in X_reg.columns if col != 'synthetic_cat']\n",
        "cat_features_reg = ['synthetic_cat']\n",
        "num_features_clf = [col for col in X_clf.columns if col != 'synthetic_cat']\n",
        "cat_features_clf = ['synthetic_cat']\n",
        "\n",
        "# Preprocessing pipelines\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# ColumnTransformer for regression\n",
        "preprocessor_reg = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, num_features_reg),\n",
        "        ('cat', categorical_transformer, cat_features_reg)\n",
        "    ])\n",
        "\n",
        "# ColumnTransformer for classification\n",
        "preprocessor_clf = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, num_features_clf),\n",
        "        ('cat', categorical_transformer, cat_features_clf)\n",
        "    ])\n",
        "\n",
        "# Split regression data\n",
        "X_reg_temp, X_reg_test, y_reg_temp, y_reg_test = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.15, random_state=RANDOM_STATE\n",
        ")\n",
        "X_reg_train, X_reg_val, y_reg_train, y_reg_val = train_test_split(\n",
        "    X_reg_temp, y_reg_temp, test_size=0.1765, random_state=RANDOM_STATE\n",
        ")  # 0.1765 of 85% = 15% of total\n",
        "\n",
        "# Split classification data (stratified)\n",
        "X_clf_temp, X_clf_test, y_clf_temp, y_clf_test = train_test_split(\n",
        "    X_clf, y_clf, test_size=0.15, random_state=RANDOM_STATE, stratify=y_clf\n",
        ")\n",
        "X_clf_train, X_clf_val, y_clf_train, y_clf_val = train_test_split(\n",
        "    X_clf_temp, y_clf_temp, test_size=0.1765, random_state=RANDOM_STATE, stratify=y_clf_temp\n",
        ")\n",
        "\n",
        "print(f'Regression splits: Train={X_reg_train.shape}, Val={X_reg_val.shape}, Test={X_reg_test.shape}')\n",
        "print(f'Classification splits: Train={X_clf_train.shape}, Val={X_clf_val.shape}, Test={X_clf_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modeling\n",
        "\n",
        "- **Regression**: Use LinearRegression to predict house prices.\n",
        "- **Classification**: Use LogisticRegression for breast cancer diagnosis (solver='lbfgs', max_iter=1000 for convergence).\n",
        "- Tune LogisticRegression hyperparameter `C` (inverse regularization strength) on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Regression pipeline\n",
        "reg_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor_reg),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Fit regression model\n",
        "reg_pipeline.fit(X_reg_train, y_reg_train)\n",
        "\n",
        "# Classification pipeline\n",
        "clf_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor_clf),\n",
        "    ('classifier', LogisticRegression(solver='lbfgs', max_iter=1000, random_state=RANDOM_STATE))\n",
        "])\n",
        "\n",
        "# Hyperparameter tuning for LogisticRegression\n",
        "param_grid = {'classifier__C': [0.01, 0.1, 1.0, 10.0]}\n",
        "grid_search = GridSearchCV(clf_pipeline, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
        "grid_search.fit(X_clf_train, y_clf_train)\n",
        "\n",
        "# Best model\n",
        "clf_pipeline = grid_search.best_estimator_\n",
        "print(f'Best C for LogisticRegression: {grid_search.best_params_[\"classifier__C\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation\n",
        "\n",
        "- **Regression Metrics**: Mean Absolute Error (MAE), Mean Squared Error (MSE), R² score.\n",
        "- **Classification Metrics**: Accuracy, precision, recall, F1 score; confusion matrix and classification report.\n",
        "- **Visualizations**: Learning curve for LogisticRegression, residuals plot for regression, ROC curve for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
        "from sklearn.model_selection import learning_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Regression evaluation\n",
        "y_reg_pred_val = reg_pipeline.predict(X_reg_val)\n",
        "y_reg_pred_test = reg_pipeline.predict(X_reg_test)\n",
        "\n",
        "reg_metrics = {\n",
        "    'MAE': [mean_absolute_error(y_reg_val, y_reg_pred_val), mean_absolute_error(y_reg_test, y_reg_pred_test)],\n",
        "    'MSE': [mean_squared_error(y_reg_val, y_reg_pred_val), mean_squared_error(y_reg_test, y_reg_pred_test)],\n",
        "    'R2': [r2_score(y_reg_val, y_reg_pred_val), r2_score(y_reg_test, y_reg_pred_test)]\n",
        "}\n",
        "reg_metrics_df = pd.DataFrame(reg_metrics, index=['Validation', 'Test'])\n",
        "print('Regression Metrics:')\n",
        "print(reg_metrics_df)\n",
        "\n",
        "# Classification evaluation\n",
        "y_clf_pred_val = clf_pipeline.predict(X_clf_val)\n",
        "y_clf_pred_test = clf_pipeline.predict(X_clf_test)\n",
        "\n",
        "clf_metrics = {\n",
        "    'Accuracy': [accuracy_score(y_clf_val, y_clf_pred_val), accuracy_score(y_clf_test, y_clf_pred_test)],\n",
        "    'Precision': [precision_score(y_clf_val, y_clf_pred_val, pos_label=1), precision_score(y_clf_test, y_clf_pred_test, pos_label=1)],\n",
        "    'Recall': [recall_score(y_clf_val, y_clf_pred_val, pos_label=1), recall_score(y_clf_test, y_clf_pred_test, pos_label=1)],\n",
        "    'F1': [f1_score(y_clf_val, y_clf_pred_val, pos_label=1), f1_score(y_clf_test, y_clf_pred_test, pos_label=1)]\n",
        "}\n",
        "clf_metrics_df = pd.DataFrame(clf_metrics, index=['Validation', 'Test'])\n",
        "print('\\nClassification Metrics:')\n",
        "print(clf_metrics_df)\n",
        "\n",
        "# Classification report\n",
        "print('\\nClassification Report (Test Set):')\n",
        "print(classification_report(y_clf_test, y_clf_pred_test, target_names=breast_cancer.target_names))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_clf_test, y_clf_pred_test)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=breast_cancer.target_names)\n",
        "disp.plot()\n",
        "plt.title('Confusion Matrix (Test Set)')\n",
        "plt.show()\n",
        "\n",
        "# Residuals plot for regression\n",
        "residuals = y_reg_test - y_reg_pred_test\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=y_reg_pred_test, y=residuals)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals vs Predicted (Regression - Test Set)')\n",
        "plt.show()\n",
        "\n",
        "# ROC curve for classification\n",
        "y_clf_prob_test = clf_pipeline.predict_proba(X_clf_test)[:, 1]\n",
        "fpr, tpr, _ = roc_curve(y_clf_test, y_clf_prob_test, pos_label=1)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve (Classification - Test Set)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# Learning curve for classification\n",
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    clf_pipeline, X_clf_train, y_clf_train, cv=5, scoring='f1', n_jobs=-1,\n",
        "    train_sizes=np.linspace(0.1, 1.0, 5), random_state=RANDOM_STATE\n",
        ")\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(train_sizes, train_scores.mean(axis=1), label='Training score')\n",
        "plt.plot(train_sizes, val_scores.mean(axis=1), label='Validation score')\n",
        "plt.xlabel('Training Set Size')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.title('Learning Curve (LogisticRegression)')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results\n",
        "\n",
        "### Regression\n",
        "- **Metrics**: The R² scores (~0.6) indicate moderate predictive power for LinearRegression. MAE and MSE are consistent between validation and test sets, suggesting no significant overfitting.\n",
        "- **Residuals Plot**: Residuals are scattered around zero but show some patterns, indicating potential non-linear relationships not captured by LinearRegression.\n",
        "\n",
        "### Classification\n",
        "- **Metrics**: High accuracy, precision, recall, and F1 scores (~0.95–0.98) show strong performance. The test set metrics are close to validation, indicating good generalization.\n",
        "- **Confusion Matrix**: Few misclassifications, with high true positives for the benign class (positive label).\n",
        "- **ROC Curve**: AUC close to 1.0 confirms excellent discrimination between classes.\n",
        "- **Learning Curve**: The training and validation F1 scores converge at larger training sizes, suggesting the model is well-fit with no significant overfitting or underfitting.\n",
        "\n",
        "### Feature Scaling\n",
        "- Scaling (via StandardScaler) was critical for LogisticRegression to ensure features contribute equally to the model, improving convergence and performance.\n",
        "\n",
        "### Class Imbalance\n",
        "- The Breast Cancer dataset is slightly imbalanced (~63% benign, 37% malignant). Stratified splitting and F1 score focus mitigated bias toward the majority class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion and Next Steps\n",
        "\n",
        "### Summary\n",
        "- The notebook successfully demonstrated regression (LinearRegression) and classification (LogisticRegression) workflows using scikit-learn Pipelines.\n",
        "- Preprocessing handled missing values and scaling, though datasets had no missing data. A synthetic categorical feature showcased encoding.\n",
        "- Models performed well, but LinearRegression may benefit from capturing non-linear patterns.\n",
        "\n",
        "### Next Steps\n",
        "1. **Regularized Models**: Try Ridge or Lasso regression to improve LinearRegression by addressing potential multicollinearity in the California Housing dataset.\n",
        "2. **Feature Engineering**: Create interaction terms or polynomial features for regression to capture non-linear relationships.\n",
        "3. **Cross-Validation**: Implement k-fold cross-validation for more robust hyperparameter tuning and performance estimation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}