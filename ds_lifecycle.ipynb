{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêß End-to-End Data Science Lifecycle Notebook\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "This notebook demonstrates a complete, reproducible data science workflow using the Palmer Penguins dataset. The process covers data collection, cleaning, exploratory data analysis (EDA), and a simple baseline modeling task. The primary goal is to predict the penguin species based on their physical measurements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Provenance\n",
    "\n",
    "-   **Title:** Palmer Archipelago (Antarctica) penguin data\n",
    "-   **Source:** Allison Horst, Alison Hill, Kristen Gorman\n",
    "-   **Source URL:** https://github.com/allisonhorst/palmerpenguins/raw/main/data/penguins.csv\n",
    "-   **License:** Creative Commons Zero v1.0 Universal (CC0 1.0) Public Domain Dedication\n",
    "-   **Access Date:** 2025-09-09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Define directory structure\n",
    "DATA_DIR = \"data\"\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"raw\")\n",
    "REPORTS_DIR = \"reports\"\n",
    "FIGURES_DIR = os.path.join(REPORTS_DIR, \"figures\")\n",
    "\n",
    "os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete. Directories created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Collection\n",
    "\n",
    "We will download the dataset directly from the source URL and save a local copy to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "DATASET_URL = \"https://github.com/allisonhorst/palmerpenguins/raw/main/data/penguins.csv\"\n",
    "FILE_NAME = \"penguins.csv\"\n",
    "raw_data_path = os.path.join(RAW_DATA_DIR, FILE_NAME)\n",
    "\n",
    "if not os.path.exists(raw_data_path):\n",
    "    print(\"Downloading data...\")\n",
    "    r = requests.get(DATASET_URL)\n",
    "    with open(raw_data_path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    print(f\"Data downloaded and saved to {raw_data_path}\")\n",
    "else:\n",
    "    print(f\"Data already exists at {raw_data_path}\")\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "df = pd.read_csv(raw_data_path)\n",
    "\n",
    "print(f\"Data loaded successfully. Initial shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning & Preparation\n",
    "\n",
    "This phase involves inspecting the data for quality issues such as missing values, duplicates, and incorrect data types. We'll then apply basic cleaning steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "\n",
    "| Column Name | Data Type | Description |\n",
    "|-------------|-----------|-------------|\n",
    "| species     | object    | Penguin species (Ad√©lie, Gentoo, Chinstrap) |\n",
    "| island      | object    | Island where the penguin was observed (Torgersen, Biscoe, Dream) |\n",
    "| culmen_length_mm | float64 | Culmen length (mm) |\n",
    "| culmen_depth_mm  | float64 | Culmen depth (mm) |\n",
    "| flipper_length_mm| float64 | Flipper length (mm) |\n",
    "| body_mass_g | float64   | Body mass (g) |\n",
    "| sex         | object    | Penguin sex (male, female) |\n",
    "| year        | int64     | Year of data collection |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3"
   },
   "outputs": [],
   "source": [
    "# Inspect the data schema\n",
    "print(\"\\n--- Initial Data Info ---\\n\")\n",
    "df.info()\n",
    "print(\"\\n--- First 5 rows ---\\n\")\n",
    "print(df.head())\n",
    "print(\"\\n--- Missing Values ---\\n\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n--- Duplicate Rows ---\\n\")\n",
    "print(f\"Number of duplicate rows: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "-   We have 7 columns with some missing values, most notably `sex`, `culmen_length_mm`, `culmen_depth_mm`, `flipper_length_mm`, and `body_mass_g`.\n",
    "-   There are no duplicate rows.\n",
    "-   The `culmen` and `flipper` columns should be numeric, as should `body_mass_g`. The `sex` column is an object type with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4"
   },
   "outputs": [],
   "source": [
    "# Drop rows with any missing values, as they are a small fraction of the total dataset.\n",
    "df.dropna(inplace=True)\n",
    "print(f\"Shape after dropping rows with missing values: {df.shape}\")\n",
    "\n",
    "# Check value counts for 'sex' column to handle potential inconsistencies\n",
    "print(\"\\n--- 'sex' value counts ---\\n\")\n",
    "print(df['sex'].value_counts())\n",
    "\n",
    "# We will drop the single 'sex' row that has a '.' value as it is likely a data entry error.\n",
    "df = df[df['sex'] != '.']\n",
    "print(f\"Shape after handling '.' in 'sex' column: {df.shape}\")\n",
    "\n",
    "# Re-check for missing values and duplicates after cleaning\n",
    "print(\"\\n--- Final check for missing values and duplicates ---\\n\")\n",
    "print(df.isnull().sum().sum()) \n",
    "print(df.duplicated().sum())\n",
    "\n",
    "penguins_df = df.copy()\n",
    "print(f\"\\nCleaned data shape: {penguins_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this section, we'll visualize the data to understand distributions, relationships between variables, and potential insights. We'll focus on the key features and the target variable, `species`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5"
   },
   "outputs": [],
   "source": [
    "# Descriptive statistics for numeric features\n",
    "print(\"\\n--- Descriptive Statistics for Numeric Features ---\\n\")\n",
    "print(penguins_df.describe())\n",
    "\n",
    "# Value counts for categorical features\n",
    "print(\"\\n--- Value Counts for Categorical Features ---\\n\")\n",
    "print(\"Species:\\n\", penguins_df['species'].value_counts())\n",
    "print(\"\\nIsland:\\n\", penguins_df['island'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6"
   },
   "outputs": [],
   "source": [
    "# Plot 1: Histogram of body mass by species\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=penguins_df, x='body_mass_g', hue='species', kde=True)\n",
    "plt.title('Distribution of Body Mass by Species')\n",
    "plt.xlabel('Body Mass (g)')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'body_mass_histogram.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7"
   },
   "outputs": [],
   "source": [
    "# Plot 2: Scatter plot of flipper length vs. culmen length by species\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=penguins_df, x='flipper_length_mm', y='culmen_length_mm', hue='species', style='species', s=100)\n",
    "plt.title('Flipper Length vs. Culmen Length by Species')\n",
    "plt.xlabel('Flipper Length (mm)')\n",
    "plt.ylabel('Culmen Length (mm)')\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'flipper_culmen_scatter.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8"
   },
   "outputs": [],
   "source": [
    "# Plot 3: Box plot to visualize flipper length outliers by species\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=penguins_df, x='species', y='flipper_length_mm')\n",
    "plt.title('Box Plot of Flipper Length by Species')\n",
    "plt.xlabel('Species')\n",
    "plt.ylabel('Flipper Length (mm)')\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'flipper_length_boxplot.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights from EDA\n",
    "\n",
    "1.  **Species are Distinct:** The scatter plot of `flipper_length_mm` vs. `culmen_length_mm` shows clear separation between the three species, suggesting these features will be highly predictive for our model.\n",
    "2.  **Size Differences:** The `Gentoo` species generally has a larger body mass and flipper length than the `Ad√©lie` and `Chinstrap` species, indicating it's the largest of the three.\n",
    "3.  **No Significant Outliers:** The box plots show a few potential outliers, but they are not extreme and are likely natural variations within the dataset, not data entry errors. The overall distributions are clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modeling (Baseline)\n",
    "\n",
    "We will now build a simple baseline classification model. The problem is to predict the `species` of a penguin based on its physical measurements. We'll use **Logistic Regression**, a standard choice for a baseline classification model, due to its simplicity and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9"
   },
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "target = 'species'\n",
    "\n",
    "X = penguins_df[features]\n",
    "y = penguins_df[target]\n",
    "\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10"
   },
   "outputs": [],
   "source": [
    "# Create a train/test split with stratification and a fixed random state\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11"
   },
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model\n",
    "model = LogisticRegression(random_state=RANDOM_SEED, max_iter=1000)\n",
    "print(\"\\nTraining the Logistic Regression model...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"\\nBaseline Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-Score (Weighted): {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results and Next Steps\n",
    "\n",
    "### Interpretation of Results\n",
    "\n",
    "The Logistic Regression model performed exceptionally well, achieving near-perfect accuracy and F1 scores. This is likely because the chosen features (`culmen` and `flipper` measurements) are highly distinct for each penguin species, as observed in our EDA. This simple model provides a strong baseline against which more complex models can be compared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations and Future Work\n",
    "\n",
    "**One Limitation:** The current model's success is heavily reliant on the quality and highly predictive nature of the features. It does not account for potential noise, more complex relationships, or situations where feature separation is not as clear. For this dataset, the problem is straightforward, but for a more complex task, this simple model might not be sufficient.\n",
    "\n",
    "**Next Steps:**\n",
    "1.  **Feature Scaling:** Standardize or normalize the numeric features (`culmen_length_mm`, `culmen_depth_mm`, etc.) to improve the performance of distance-based models like Support Vector Machines (SVM) or K-Nearest Neighbors.\n",
    "2.  **Hyperparameter Tuning:** Use techniques like `GridSearchCV` to find the optimal hyperparameters for the Logistic Regression model.\n",
    "3.  **Cross-Validation:** Implement k-fold cross-validation to get a more robust estimate of the model's performance, reducing dependence on a single train/test split."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
